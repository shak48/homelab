###Temporary change editor to edit secrets
export EDITOR=nano


Proxmox api viewer: https://pve.proxmox.com/pve-docs-8/api-viewer/index.html

# Boot tool status
    proxmox-boot-tool status

# zpool
# man zpoolconcepts
    zpool status 
    zfs list
    zpool status -v <pool-name>

    zpool create mypool mirror sda sdb mirror sdc sdd
    
# Hot Spares
     ZFS allows devices to be associated with pools as "hot spares".  These devices are not
     actively used in the pool.  But, when an active device fails, it is automatically re‐
     placed by a hot spare.  To create a pool with hot spares, specify a spare vdev with any
     number of devices.  For example,
           # zpool create pool mirror sda sdb spare sdc sdd
# Spare
     Spares can be shared across multiple pools, and can be added with the zpool add command
     and removed with the zpool remove command.  Once a spare replacement is initiated, a new
     spare vdev is created within the configuration that will remain there until the original
     device is replaced.  At this point, the hot spare becomes available again, if another
     device fails.

     To create a pool with cache devices, specify a cache vdev with any number of devices.
     For example:
           # zpool create pool sda sdb cache sdc sdd
   Pool checkpoint
     Before starting critical procedures that include destructive actions (like zfs destroy),
     an administrator can checkpoint the pool's state and, in the case of a mistake or fail‐
     ure, rewind the entire pool back to the checkpoint.  Otherwise, the checkpoint can be
     discarded when the procedure has completed successfully.
# Checkpoint
     A pool checkpoint can be thought of as a pool-wide snapshot and should be used with care.

     To create a checkpoint for a pool:
           # zpool checkpoint pool

     To later rewind to its checkpointed state, you need to first export it and then rewind
     it during import:
           # zpool export pool
           # zpool import --rewind-to-checkpoint pool

     To discard the checkpoint from a pool:
           # zpool checkpoint -d pool
# man zpool
# man zfs

# Create a new pool with RAID-1
    zpool create -f -o ashift=12 <pool> mirror <device1> <device2>
# Create ZFS pool with a on-disk cache
    zpool create -f -o ashift=12 <pool> <device> ca

# Changing a failed device
    zpool replace -f <pool> <old-device> <new-device>

# ZFS comes with an event daemon ZED, which monitors events generated by the ZFS kernel module. The daemon can also send emails on ZFS events like pool errors. Newer ZFS packages ship the daemon in a separate zfs-zed package, which should already be installed by default in Proxmox VE. You can configure the daemon via the file /etc/zfs/zed.d/zed.rc with your favorite editor. Therequired setting for email notification is ZED_EMAIL_ADDR, which is set to root by default.

    ZED_EMAIL_ADDR="root"


# File systems
    Disk images for VMs are stored in ZFS volume (zvol) datasets, which provide block device functionality.
    On file based storages, snapshots are possible with the qcow2 format, either using the internal snapshot
    function, or snapshots as volume chains4.
    It is possible to use LVM on top of an iSCSI or FC-based storage. That way you get a shared LVM storage
    
# Common Storage Properties
    A few storage properties are common among different storage types.
    
    images
        QEMU/KVM VM images.
    rootdir
        Allow to store container data.
    vztmpl
        Container templates.
    backup
        Backup files (vzdump).


# Volumes
    We use a special notation to address storage data. When you allocate data from a storage pool, it returns such a volume identifier. A volume is identified by the <STORAGE_ID>, followed by a storage type
    dependent volume name, separated by colon. A valid <VOLUME_ID> looks like:
    local:230/example-image.raw
    local:iso/debian-501-amd64-netinst.iso
    local:vztmpl/debian-5.0-joomla_1.5.9-1_i386.tar.gz
    iscsi-storage:0.0.2.scsi-14 ←-
    f504e46494c4500494b5042546d2d646744372d31616d61

    pvesm path <VOLUME_ID>

# Volume Ownership
    There exists an ownership relation for image type volumes. Each such volume is owned by a VM or
    Container. For example volume local:230/example-image.raw is owned by VM 230. Most storage
    backends encodes this ownership information into the volume name.
    When you remove a VM or Container, the system also removes all associated volumes which are owned by
    that VM or Container.

# Volume Examples
    Add storage pools
        pvesm add <TYPE> <STORAGE_ID> <OPTIONS>
        pvesm add dir <STORAGE_ID> --path <PATH>
        pvesm add nfs <STORAGE_ID> --path <PATH> --server <SERVER> --export <EXPORT>
        pvesm add lvm <STORAGE_ID> --vgname <VGNAME>
        pvesm add iscsi <STORAGE_ID> --portal <HOST[:PORT]> --target <TARGET 
    
    Disable storage pools
        pvesm set <STORAGE_ID> --disable 1
    
    Enable storage pools
        pvesm set <STORAGE_ID> --disable 0
    
    Change/set storage options
        pvesm set <STORAGE_ID> <OPTIONS>
        pvesm set <STORAGE_ID> --shared 1
        pvesm set local --format qcow2
        pvesm set <STORAGE_ID> --content iso
    
    Remove storage pools. This does not delete any data, and does not disconnect or unmount anything. It just removes the storage configuration.
        pvesm remove <STORAGE_ID>

    Allocate volumes
        pvesm alloc <STORAGE_ID> <VMID> <name> <size> [--format <raw|qcow2>]
    
    Allocate a 4G volume in local storage. The name is auto-generated if you pass an empty string as <name>
        pvesm alloc local <VMID> '' 4G
    
    Free volumes
        pvesm free <VOLUME_ID>

    List storage status
        pvesm status
    
    List storage contents
        pvesm list <STORAGE_ID> [--vmid <VMID>]
    
    List volumes allocated by VMID
        pvesm list <STORAGE_ID> --vmid <VMID>
    
    List iso images
        pvesm list <STORAGE_ID> --content iso
    
    List container templates
        pvesm list <STORAGE_ID> --content vztmpl
    
    Show file system path for a volume
        pvesm path <VOLUME_ID>
    
    Exporting the volume local:103/vm-103-disk-0.qcow2 to the file target. This is mostly used internally with pvesm import. The stream format qcow2+size is different to the qcow2 format. Consequently, the exported file cannot simply be attached to a VM. This also holds for the other formats.
        pvesm export local:103/vm-103-disk-0.qcow2 qcow2+size target --with snapshots 1